!pip install pandas numpy matplotlib seaborn scikit-learn pgmpy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx

from scipy import stats
from scipy.stats import ttest_ind

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (confusion_matrix, classification_report,
                            accuracy_score, precision_score, recall_score,
                            f1_score, roc_auc_score, roc_curve)
from sklearn.naive_bayes import GaussianNB

from pgmpy.models import BayesianNetwork, DiscreteBayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator, HillClimbSearch
from pgmpy.estimators.StructureScore import BIC
from pgmpy.inference import VariableElimination

import warnings
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
%matplotlib inline

print("="*80)
print("PIMA INDIANS DIABETES PREDICTION USING BAYESIAN NETWORK")
print("="*80)

# Define a consistent palette for the whole project

# Binary colors (Negative/Positive, 0/1, No-Risk/Risk)
COLOR_NEG = '#3498db'  # Blue (No Diabetes)
COLOR_POS = '#e74c3c'  # Red (Diabetes)
PALETTE_BINARY = [COLOR_NEG, COLOR_POS]

# Neutral color for single-feature distributions
COLOR_HIST = '#3498db' # Blue

# Color for network graph nodes
COLOR_NODE = '#add8e6' # Light Blue

# Colormap for correlation (Diverging)
CMAP_DIVERGING = 'coolwarm' # Red/Blue

# Colormap for feature importance (Sequential)
CMAP_IMPORTANCE = 'viridis'

# Colormap for threshold optimization (Qualitative, distinct lines)
PALETTE_THRESHOLD = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

# Colormaps for Confusion Matrices
CMAP_CONF_COUNT = 'Blues'
CMAP_CONF_PCT = 'Reds'

print("\n" + "="*80)
print("1. DATA LOADING AND INITIAL EXPLORATION")
print("="*80)

df = pd.read_csv('/content/diabetes.csv')

print(f"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns")
df.head()

print("="*70)
print("DATASET OVERVIEW")
print("="*70)
print(f"\nShape: {df.shape[0]} samples, {df.shape[1]} features\n")

print("Column Datatype Information:")
print(df.info())

print("\nStatistical Summary:")
print(df.describe())

print("\nMissing Values:")
print(df.isnull().sum())

print("\nTarget Variable Distribution:")
print(df['Outcome'].value_counts())
print("\nClass Proportions:")
print(df['Outcome'].value_counts(normalize=True) * 100)

print("\n" + "="*80)
print("2. DATA QUALITY ANALYSIS")
print("="*80)

# Check missing values
print("\nMissing Values Check:")
missing = df.isnull().sum()
missing_pct = (missing / len(df)) * 100
missing_df = pd.DataFrame({
    'Missing_Count': missing,
    'Percentage': missing_pct
})
missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Percentage', ascending=False)

if len(missing_df) > 0:
    print(missing_df)
    msno.matrix(df)
    plt.title('Missing Value Pattern', fontsize=14, fontweight='bold')
    plt.show()
else:
    print("No missing values found.")

# Check duplicates
duplicates = df.duplicated().sum()
print(f"\nDuplicate Rows: {duplicates}")
if duplicates > 0:
    print(f"   Removing {duplicates} duplicate rows...")
    df = df.drop_duplicates()

# Check for zeros in features where zero is physiologically impossible
zero_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
print("\nZeros in features (likely missing values):")
for feature in zero_features:
    zero_count = (df[feature] == 0).sum()
    zero_pct = (zero_count / len(df)) * 100
    print(f"  {feature:25s}: {zero_count:3d} ({zero_pct:5.2f}%)")

# Data type summary
print("\nData Types Summary:")
print(df.dtypes.value_counts())

print("\n" + "="*80)
print("3. EXPLORATORY DATA ANALYSIS")
print("="*80)

# Target Variable Distribution
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

target_counts = df['Outcome'].value_counts()
axes[0].bar(['No Diabetes (0)', 'Diabetes (1)'], target_counts.values,
            color=PALETTE_BINARY) # MODIFIED
axes[0].set_title('Diabetes Outcome Distribution', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Count', fontsize=12)
axes[0].grid(axis='y', alpha=0.3)
for i, v in enumerate(target_counts.values):
    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')

axes[1].pie(target_counts.values, labels=['No Diabetes (0)', 'Diabetes (1)'],
            autopct='%1.1f%%', colors=PALETTE_BINARY, startangle=90) # MODIFIED
axes[1].set_title('Diabetes Outcome Proportion', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig('01_target_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# Feature Distributions
features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
            'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']

fig, axes = plt.subplots(3, 3, figsize=(16, 12))
axes = axes.ravel()

for idx, col in enumerate(features):
    axes[idx].hist(df[col], bins=30, color=COLOR_HIST, # MODIFIED
                   edgecolor='black', alpha=0.7)
    axes[idx].set_title(f'{col} Distribution', fontsize=11, fontweight='bold')
    axes[idx].set_xlabel(col, fontsize=10)
    axes[idx].set_ylabel('Frequency', fontsize=10)
    axes[idx].grid(axis='y', alpha=0.3)

# Remove empty subplot
fig.delaxes(axes[-1])

plt.tight_layout()
plt.savefig('02_feature_distributions.png', dpi=300, bbox_inches='tight')
plt.show()

# Box plots by outcome
fig, axes = plt.subplots(3, 3, figsize=(16, 12))
axes = axes.ravel()

for idx, col in enumerate(features):
    sns.boxplot(x='Outcome', y=col, data=df, ax=axes[idx], palette=PALETTE_BINARY)

    axes[idx].set_title(f'{col} by Diabetes Outcome',
                        fontsize=11, fontweight='bold')
    axes[idx].set_xlabel('Diabetes Outcome', fontsize=10)
    axes[idx].set_ylabel(col, fontsize=10)
    axes[idx].set_xticklabels(['No Diabetes (0)', 'Diabetes (1)'])

fig.delaxes(axes[-1])
plt.tight_layout()
plt.savefig('03_boxplots_by_outcome.png', dpi=300, bbox_inches='tight')
plt.show()

# Correlation Analysis
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap=CMAP_DIVERGING, # MODIFIED
            center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix - All Features', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('04_correlation_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# Correlation with target
target_correlation = correlation_matrix['Outcome'].sort_values(ascending=False)
print("\nCorrelation with Outcome:")
print(target_correlation)

plt.figure(figsize=(10, 6))

# Drop 'Outcome' itself (corr=1.0) for a cleaner plot
target_corr_features = target_correlation.drop('Outcome')

# Assign colors based on positive (Risk) or negative (No Risk)
colors = [COLOR_POS if x > 0 else COLOR_NEG for x in target_corr_features.values]

plt.barh(target_corr_features.index, target_corr_features.values, color=colors)
plt.xlabel('Correlation Coefficient', fontsize=12)
plt.title('Feature Correlation with Diabetes Outcome',
          fontsize=14, fontweight='bold')
plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('05_target_correlation.png', dpi=300, bbox_inches='tight')
plt.show()

# Strong inter-feature correlations
print("\nStrong Inter-Feature Correlations (|r| > 0.3):")
strong_corr = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.3:
            strong_corr.append({
                'Feature 1': correlation_matrix.columns[i],
                'Feature 2': correlation_matrix.columns[j],
                'Correlation': correlation_matrix.iloc[i, j],
                'BN_Implication': 'Potential edge or common parent'
            })

strong_corr_df = pd.DataFrame(strong_corr).sort_values('Correlation', key=abs, ascending=False)
print(strong_corr_df.head(20).to_string(index=False))  

# Feature Importance Analysis Using Random Forest

# Prepare data for Random Forest
X = df.drop('Outcome', axis=1).copy()
y = df['Outcome']
X = X.fillna(X.median())

# Train Random Forest
rf = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1)
rf.fit(X, y)

# Get feature importance
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nImportance of Features:")
print(importance_df.head(8).to_string(index=False))

# Visualize
plt.figure(figsize=(14, 8))
top_features = importance_df.head(15)
sns.barplot(data=top_features, x='Importance', y='Feature', palette=CMAP_IMPORTANCE) # MODIFIED
plt.title('Feature Importance (Random Forest)', fontsize=16, fontweight='bold')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

print("\n" + "="*80)
print("4. DATA PREPROCESSING")
print("="*80)

# Create a copy for preprocessing
df_processed = df.copy()

# Handle zeros as missing values for specific features
print("\nHandling zeros as missing values...")
for feature in zero_features:
    zero_count = (df_processed[feature] == 0).sum()
    if zero_count > 0:
        # Replace zeros with median
        median_val = df_processed[df_processed[feature] != 0][feature].median()
        df_processed.loc[df_processed[feature] == 0, feature] = median_val
        print(f"  {feature}: Replaced {zero_count} zeros with median ({median_val:.2f})")

print("\nPreprocessing completed!")

print("\n" + "="*80)
print("5. FEATURE DISCRETIZATION")
print("="*80)

df_discrete = df_processed.copy()

# Discretize continuous features based on medical/statistical thresholds

# Age: Standard age groups
df_discrete['Age'] = pd.cut(df_discrete['Age'],
                            bins=[0, 30, 40, 50, 100],
                            labels=['Young', 'Middle', 'Senior', 'Elderly'])

# Pregnancies: Group by frequency
df_discrete['Pregnancies'] = pd.cut(df_discrete['Pregnancies'],
                                   bins=[-1, 0, 3, 6, 20],
                                   labels=['None', 'Low', 'Medium', 'High'])

# Glucose: Based on diabetes diagnosis thresholds
df_discrete['Glucose'] = pd.cut(df_discrete['Glucose'],
                               bins=[0, 100, 125, 200],
                               labels=['Normal', 'Prediabetes', 'Diabetes'])

# Blood Pressure: Clinical categories
df_discrete['BloodPressure'] = pd.cut(df_discrete['BloodPressure'],
                                     bins=[0, 80, 90, 200],
                                     labels=['Normal', 'Elevated', 'High'])

# BMI: WHO categories
df_discrete['BMI'] = pd.cut(df_discrete['BMI'],
                           bins=[0, 18.5, 25, 30, 100],
                           labels=['Underweight', 'Normal', 'Overweight', 'Obese'])

# Skin Thickness: Use cut with explicit bins to avoid duplicate issues
skin_thickness_bins = [0, 20, 30, 100]
df_discrete['SkinThickness'] = pd.cut(df_discrete['SkinThickness'],
                                     bins=skin_thickness_bins,
                                     labels=['Low', 'Medium', 'High'],
                                     include_lowest=True)

# Insulin: Use cut with explicit bins (based on data distribution)
insulin_bins = [0, 100, 200, 900]
df_discrete['Insulin'] = pd.cut(df_discrete['Insulin'],
                               bins=insulin_bins,
                               labels=['Low', 'Medium', 'High'],
                               include_lowest=True)

# Diabetes Pedigree Function: Use percentiles with fallback
try:
    df_discrete['DiabetesPedigreeFunction'] = pd.qcut(
        df_discrete['DiabetesPedigreeFunction'],
        q=3,
        labels=['Low', 'Medium', 'High'],
        duplicates='drop'
    )
except ValueError:
    # Fallback to cut if qcut fails
    dpf_bins = [0, 0.3, 0.6, 3.0]
    df_discrete['DiabetesPedigreeFunction'] = pd.cut(
        df_discrete['DiabetesPedigreeFunction'],
        bins=dpf_bins,
        labels=['Low', 'Medium', 'High'],
        include_lowest=True
    )

print("\nDiscretization complete!")

print("\nSample of discretized data:")
print(df_discrete.head(10))

print("\nDiscretized feature statistics:")
for col in df_discrete.columns:
    print(f"  {col:30s}: {df_discrete[col].nunique()} unique values")

print("\n" + "="*80)
print("6. TRAIN-TEST SPLIT")
print("="*80)

# Convert all to string for pgmpy
for col in df_discrete.columns:
    df_discrete[col] = df_discrete[col].astype(str)

# Separate features and target
X = df_discrete.drop('Outcome', axis=1)
y = df_discrete['Outcome']

# Split with stratification and a fixed random_state for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2,
    random_state=24,
    stratify=y
)

print(f"\nTraining set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")

print("\nTarget distribution in training set:")
train_counts = y_train.value_counts().sort_index()
print(train_counts)

print("\nTarget distribution in test set:")
test_counts = y_test.value_counts().sort_index()
print(test_counts)

# Visualize split
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

labels = ['No Diabetes (0)', 'Diabetes (1)']
axes[0].bar(labels, train_counts.values, color=PALETTE_BINARY)
axes[0].set_title('Training Set Distribution', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Count', fontsize=12)
axes[0].grid(axis='y', alpha=0.3)
for i, v in enumerate(train_counts.values):
    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold') # Adjusted spacing slightly

test_counts = y_test.value_counts().sort_index()
axes[1].bar(labels, test_counts.values, color=PALETTE_BINARY)
axes[1].set_title('Test Set Distribution', fontsize=14, fontweight='bold')
axes[1].set_ylabel('Count', fontsize=12)
axes[1].grid(axis='y', alpha=0.3)
for i, v in enumerate(test_counts.values):
    axes[1].text(i, v + 3, str(v), ha='center', fontweight='bold') # Adjusted spacing slightly

plt.tight_layout()
plt.savefig('08_train_test_split.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*80)
print("7. BAYESIAN NETWORK MODEL CONSTRUCTION (with Structure Learning)")
print("="*80)

# Combine train data
train_data = pd.concat([X_train, y_train], axis=1)

# Scoring method
bic = BIC(data=train_data)

# Search algorithm
print("\nSearching for the best model structure...")
hc = HillClimbSearch(data=train_data)
best_model_structure = hc.estimate(scoring_method=bic, max_iter=500, tabu_length=50)

print("\n" + "="*80)
print("LEARNED NETWORK STRUCTURE (SUMMARY)")
print("="*80)

print(f"\nTotal Nodes: {len(best_model_structure.nodes())}")
print(f"Nodes: {list(best_model_structure.nodes())}")
print(f"\nTotal Edges: {len(best_model_structure.edges())}")
all_children = set([child for parent, child in best_model_structure.edges()])
root_nodes = [node for node in best_model_structure.nodes() if node not in all_children]
print(f"\nRoot Nodes (no incoming edges):")
print(f"   {', '.join(root_nodes)}")

parents_dict = {}
for parent, child in best_model_structure.edges():
    if parent not in parents_dict:
        parents_dict[parent] = []
    parents_dict[parent].append(child)

print(f"\nParent-Child Relationships (Edges):")
for parent, children in parents_dict.items():
    print(f"   {parent} -> {', '.join(children)}")

print("\n" + "="*80)
print("NETWORK VISUALIZATION")
print("="*80)

try:
    print("Generating network plot...")

    plt.figure(figsize=(15, 10))
    pos = nx.kamada_kawai_layout(best_model_structure)

    nx.draw(
        best_model_structure,
        with_labels=True,
        node_size=4500,
        node_color=COLOR_NODE,
        font_size=9,
        font_weight='bold',
        pos=pos,
        arrows=True,
        arrowstyle='-|> ',
        arrowsize=20
    )

    plt.title("Learned Bayesian Network Structure", fontsize=16)
    plt.show()

except Exception as e:
    print(f"\nError during visualization: {e}")
    print("Please ensure 'networkx' and 'matplotlib' are installed.")

# Create and fit the model
model = DiscreteBayesianNetwork(best_model_structure.edges())

print("\n\n" + "="*80)
print("MODEL FITTING")
print("="*80)
print("\nFitting Bayesian Network to the learned structure...")
model.fit(train_data, estimator=BayesianEstimator, prior_type="BDeu")
print("\nModel fitting completed successfully!")

USE_PGMPY = True
print("\n" + "="*80)
print("8. MODEL EVALUATION")
print("="*80)

if USE_PGMPY:
    print("\nMaking predictions with Bayesian Network...")
    inference = VariableElimination(model)

    predictions_proba = []
    predictions = []

    for idx in range(len(X_test)):
        if idx % 50 == 0:
            print(f"  Processed {idx}/{len(X_test)} samples...")

        evidence = {}
        for col in X_test.columns:
            val = X_test.iloc[idx][col]
            if pd.notna(val):
                evidence[col] = str(val)

        try:
            result = inference.query(
                variables=['Outcome'],
                evidence=evidence,
                show_progress=False
            )
            proba = result.values[1] if len(result.values) > 1 else result.values[0]
            predictions_proba.append(proba)
            predictions.append('1' if proba >= 0.5 else '0')
        except:
            predictions_proba.append(0.5)
            predictions.append('0')

    predictions_proba = np.array(predictions_proba)
    predictions = np.array(predictions)

else:
    predictions_proba = model.predict_proba(X_test)[:, 1]
    predictions = model.predict(X_test).astype(str)

# Convert to int for metrics
y_test_int = y_test.astype(int).values
predictions_int = np.array([int(p) for p in predictions])

# Calculate metrics
accuracy = accuracy_score(y_test_int, predictions_int)
precision = precision_score(y_test_int, predictions_int, zero_division=0)
recall = recall_score(y_test_int, predictions_int, zero_division=0)
f1 = f1_score(y_test_int, predictions_int, zero_division=0)
roc_auc = roc_auc_score(y_test_int, predictions_proba)

print(f"\n{'='*60}")
print("MODEL PERFORMANCE ON TEST SET")
print(f"{'='*60}")
print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
print(f"Recall:    {recall:.4f} ({recall*100:.2f}%)")
print(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)")
print(f"ROC-AUC:   {roc_auc:.4f} ({roc_auc*100:.2f}%)")

print("\n" + "="*80)
print("9. THRESHOLD OPTIMIZATION")
print("="*80)

thresholds = np.arange(0.1, 0.9, 0.05)
metrics_by_threshold = []

for threshold in thresholds:
    y_pred = (predictions_proba >= threshold).astype(int)

    acc = accuracy_score(y_test_int, y_pred)
    prec = precision_score(y_test_int, y_pred, zero_division=0)
    rec = recall_score(y_test_int, y_pred, zero_division=0)
    f1_t = f1_score(y_test_int, y_pred, zero_division=0)

    metrics_by_threshold.append({
        'threshold': threshold,
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1_score': f1_t
    })

metrics_df = pd.DataFrame(metrics_by_threshold)
optimal_idx = metrics_df['f1_score'].idxmax()
optimal_threshold = metrics_df.loc[optimal_idx, 'threshold']

print(f"\nOptimal Threshold: {optimal_threshold:.2f}")
print(f"F1-Score at optimal threshold: {metrics_df.loc[optimal_idx, 'f1_score']:.4f}")

plt.figure(figsize=(12, 7))
plt.plot(metrics_df['threshold'], metrics_df['accuracy'],
         marker='o', label='Accuracy', linewidth=2, color=PALETTE_THRESHOLD[0])
plt.plot(metrics_df['threshold'], metrics_df['precision'],
         marker='s', label='Precision', linewidth=2, color=PALETTE_THRESHOLD[1])
plt.plot(metrics_df['threshold'], metrics_df['recall'],
         marker='^', label='Recall', linewidth=2, color=PALETTE_THRESHOLD[2])
plt.plot(metrics_df['threshold'], metrics_df['f1_score'],
         marker='d', label='F1-Score', linewidth=2, color=PALETTE_THRESHOLD[3])
plt.axvline(x=optimal_threshold, color='dimgray', linestyle='--', linewidth=2, # Changed from red
            label=f'Optimal ({optimal_threshold:.2f})')
plt.xlabel('Threshold', fontsize=12)
plt.ylabel('Score', fontsize=12)
plt.title('Classification Metrics vs Threshold', fontsize=14, fontweight='bold')
plt.legend(loc='best', fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('06_threshold_optimization.png', dpi=300, bbox_inches='tight')
plt.show()
print("\n" + "="*80)
print("10. CONFUSION MATRIX")
print("="*80)

y_pred_optimal = (predictions_proba >= optimal_threshold).astype(int)
cm = confusion_matrix(y_test_int, y_pred_optimal)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Absolute counts
sns.heatmap(cm, annot=True, fmt='d', cmap=CMAP_CONF_COUNT, ax=axes[0], # MODIFIED
            square=True, linewidths=2,
            xticklabels=['Predicted: No Diabetes', 'Predicted: Diabetes'],
            yticklabels=['Actual: No Diabetes', 'Actual: Diabetes'])
axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')

# Percentages
cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap=CMAP_CONF_PCT, ax=axes[1], # MODIFIED
            square=True, linewidths=2,
            xticklabels=['Predicted: No Diabetes', 'Predicted: Diabetes'],
            yticklabels=['Actual: No Diabetes', 'Actual: Diabetes'])
axes[1].set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig('07_confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

tn, fp, fn, tp = cm.ravel()
print(f"\nConfusion Matrix Components:")
print(f"   True Negatives:  {tn}")
print(f"   False Positives: {fp}")
print(f"   False Negatives: {fn}")
print(f"   True Positives:  {tp}")

print("\n" + "="*80)
print("11. ROC CURVE")
print("="*80)

fpr, tpr, _ = roc_curve(y_test_int, predictions_proba)

plt.figure(figsize=(10, 8))
plt.plot(fpr, tpr, color='darkorange', lw=2,
         label=f'ROC Curve (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
         label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend(loc='lower right', fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('08_roc_curve.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n" + "="*80)
print("13. SAVING RESULTS")
print("="*80)

# Save predictions
results_df = pd.DataFrame({
    'Actual': y_test_int,
    'Predicted': y_pred_optimal,
    'Probability': predictions_proba,
    'Correct': (y_test_int == y_pred_optimal).astype(int)
})
results_df.to_csv('pima_predictions.csv', index=False)
print("\nPredictions saved to 'pima_predictions.csv'")

# Save metrics
metrics_summary = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],
    'Score': [
        accuracy_score(y_test_int, y_pred_optimal),
        precision_score(y_test_int, y_pred_optimal, zero_division=0),
        recall_score(y_test_int, y_pred_optimal, zero_division=0),
        f1_score(y_test_int, y_pred_optimal, zero_division=0),
        roc_auc
    ]
})
metrics_summary.to_csv('pima_metrics.csv', index=False)
print("Metrics saved to 'pima_metrics.csv'")

print("\n" + "="*80)
print("ANALYSIS COMPLETED")
print("="*80)
print(f"\nFinal Accuracy: {accuracy_score(y_test_int, y_pred_optimal)*100:.2f}%")
print(f"Final ROC-AUC: {roc_auc*100:.2f}%")
print(f"Optimal Threshold: {optimal_threshold:.2f}")
print("="*80)




!pip install pgmpy

import pandas as pd
import numpy as np
import pickle
import os
from pgmpy.models import DiscreteBayesianNetwork
from pgmpy.estimators import BayesianEstimator, HillClimbSearch, BIC
import warnings
warnings.filterwarnings('ignore')

def preprocess_data(df):
    """Preprocess the dataset - handle zeros as missing values"""
    print("Preprocessing data...")
    zero_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
    df_processed = df.copy()
    
    for feature in zero_features:
        zero_count = (df_processed[feature] == 0).sum()
        if zero_count > 0:
            median_val = df_processed[df_processed[feature] != 0][feature].median()
            df_processed.loc[df_processed[feature] == 0, feature] = median_val
            print(f"  {feature}: Replaced {zero_count} zeros with median ({median_val:.2f})")
    
    return df_processed

def discretize_data(df):
    """Discretize continuous features based on medical thresholds"""
    print("\nDiscretizing features...")
    df_discrete = df.copy()
    
    # Age: Standard age groups
    df_discrete['Age'] = pd.cut(df_discrete['Age'],
                                bins=[0, 30, 40, 50, 100],
                                labels=['Young', 'Middle', 'Senior', 'Elderly'])
    
    # Pregnancies: Group by frequency
    df_discrete['Pregnancies'] = pd.cut(df_discrete['Pregnancies'],
                                       bins=[-1, 0, 3, 6, 20],
                                       labels=['None', 'Low', 'Medium', 'High'])
    
    # Glucose: Based on diabetes diagnosis thresholds
    df_discrete['Glucose'] = pd.cut(df_discrete['Glucose'],
                                   bins=[0, 100, 125, 200],
                                   labels=['Normal', 'Prediabetes', 'Diabetes'])
    
    # Blood Pressure: Clinical categories
    df_discrete['BloodPressure'] = pd.cut(df_discrete['BloodPressure'],
                                         bins=[0, 80, 90, 200],
                                         labels=['Normal', 'Elevated', 'High'])
    
    # BMI: WHO categories
    df_discrete['BMI'] = pd.cut(df_discrete['BMI'],
                               bins=[0, 18.5, 25, 30, 100],
                               labels=['Underweight', 'Normal', 'Overweight', 'Obese'])
    
    # Skin Thickness
    df_discrete['SkinThickness'] = pd.cut(df_discrete['SkinThickness'],
                                         bins=[0, 20, 30, 100],
                                         labels=['Low', 'Medium', 'High'],
                                         include_lowest=True)
    
    # Insulin
    df_discrete['Insulin'] = pd.cut(df_discrete['Insulin'],
                                   bins=[0, 100, 200, 900],
                                   labels=['Low', 'Medium', 'High'],
                                   include_lowest=True)
    
    # Diabetes Pedigree Function
    try:
        df_discrete['DiabetesPedigreeFunction'] = pd.qcut(
            df_discrete['DiabetesPedigreeFunction'],
            q=3,
            labels=['Low', 'Medium', 'High'],
            duplicates='drop'
        )
    except ValueError:
        df_discrete['DiabetesPedigreeFunction'] = pd.cut(
            df_discrete['DiabetesPedigreeFunction'],
            bins=[0, 0.3, 0.6, 3.0],
            labels=['Low', 'Medium', 'High'],
            include_lowest=True
        )
    
    # Convert all to string for pgmpy
    for col in df_discrete.columns:
        df_discrete[col] = df_discrete[col].astype(str)
    
    print("  Discretization complete!")
    return df_discrete

def train_model(csv_path='diabetes.csv'):
    """Train the Bayesian Network model"""
    print("\n" + "="*70)
    print("BAYESIAN NETWORK MODEL TRAINING")
    print("="*70)
    
    # Load data
    print(f"\nLoading data from {csv_path}...")
    df = pd.read_csv(csv_path)
    print(f"  Loaded {df.shape[0]} rows, {df.shape[1]} columns")
    
    # Check for duplicates
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        print(f"  Removing {duplicates} duplicate rows...")
        df = df.drop_duplicates()
    
    # Preprocess
    df_processed = preprocess_data(df)
    
    # Discretize
    df_discrete = discretize_data(df_processed)
    
    # Structure learning
    print("\n" + "="*70)
    print("STRUCTURE LEARNING (Hill Climb Search + BIC)")
    print("="*70)
    print("\nThis may take 1-2 minutes...")
    
    bic = BIC(data=df_discrete)
    hc = HillClimbSearch(data=df_discrete)
    best_model_structure = hc.estimate(scoring_method=bic, max_iter=500, tabu_length=50)
    
    print(f"\n✓ Structure learned successfully!")
    print(f"  Total Nodes: {len(best_model_structure.nodes())}")
    print(f"  Total Edges: {len(best_model_structure.edges())}")
    
    # Display network structure
    all_children = set([child for parent, child in best_model_structure.edges()])
    root_nodes = [node for node in best_model_structure.nodes() if node not in all_children]
    print(f"\n  Root Nodes (no incoming edges):")
    print(f"    {', '.join(root_nodes)}")
    
    parents_dict = {}
    for parent, child in best_model_structure.edges():
        if parent not in parents_dict:
            parents_dict[parent] = []
        parents_dict[parent].append(child)
    
    print(f"\n  Parent-Child Relationships (Edges):")
    for parent, children in parents_dict.items():
        print(f"    {parent} -> {', '.join(children)}")
    
    # Create and fit model
    print("\n" + "="*70)
    print("MODEL FITTING (Bayesian Estimation)")
    print("="*70)
    
    model = DiscreteBayesianNetwork(best_model_structure.edges())
    model.fit(df_discrete, estimator=BayesianEstimator, prior_type="BDeu")
    
    print("\n✓ Model fitted successfully!")
    
    return model, best_model_structure, df_discrete

def save_model(model, structure, model_path='models/bn_model.pkl'):
    """Save the trained model"""
    print("\n" + "="*70)
    print("SAVING MODEL")
    print("="*70)
    
    # Create models directory
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    
    # Save model and structure
    model_data = {
        'model': model,
        'structure': structure
    }
    
    with open(model_path, 'wb') as f:
        pickle.dump(model_data, f)
    
    file_size_kb = os.path.getsize(model_path) / 1024
    print(f"\n✓ Model saved to: {model_path}")
    print(f"  File size: {file_size_kb:.2f} KB")
    
    return model_path

def main():
    """Main function"""
    print("\n" + "="*70)
    print("PIMA INDIANS DIABETES - BAYESIAN NETWORK TRAINING")
    print("="*70)
    print("\nThis script will:")
    print("  1. Load and preprocess diabetes.csv")
    print("  2. Discretize features based on medical thresholds")
    print("  3. Learn optimal Bayesian Network structure")
    print("  4. Train the model with Bayesian estimation")
    print("  5. Save the model to models/bn_model.pkl")
    print("\n" + "="*70)
    
    input("\nPress ENTER to start training...")
    
    try:
        # Check if dataset exists
        if not os.path.exists('diabetes.csv'):
            print("\n❌ ERROR: diabetes.csv not found!")
            print("   Please ensure diabetes.csv is in the same directory as this script.")
            return
        
        # Train model
        model, structure, data = train_model('diabetes.csv')
        
        # Save model
        model_path = save_model(model, structure, 'models/bn_model.pkl')
        
        # Success message
        print("\n" + "="*70)
        print("SUCCESS! MODEL TRAINING COMPLETED")
        print("="*70)
        print(f"\n✓ Model saved to: {model_path}")
        print(f"✓ Dataset rows: {len(data)}")
        print(f"✓ Features: {len(data.columns) - 1}")
        print(f"✓ Target: Outcome")
        
        # Display feature discretization info
        print("\n" + "="*70)
        print("FEATURE DISCRETIZATION SUMMARY")
        print("="*70)
        for col in data.columns:
            unique_vals = data[col].nunique()
            print(f"  {col:30s}: {unique_vals} categories")
        
        print("\n" + "="*70)
        print("NEXT STEPS FOR DEPLOYMENT")
        print("="*70)
        print("1. Add the trained model to your Git repository:")
        print("   git add models/bn_model.pkl")
        print("   git commit -m 'Add pre-trained Bayesian Network model'")
        print("   git push origin main")
        print("\n2. Create a Streamlit app that loads this model:")
        print("   - Load model using: pickle.load(open('models/bn_model.pkl', 'rb'))")
        print("   - Use the same discretization thresholds for user inputs")
        print("   - Make predictions using VariableElimination")
        print("\n3. Deploy to Streamlit Cloud!")
        print("="*70)
        
    except Exception as e:
        print(f"\n❌ ERROR during training: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
